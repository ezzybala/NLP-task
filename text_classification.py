# -*- coding: utf-8 -*-
"""text_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jWDgYHgzbdRzyv3FzfRGMGQSMjcE5h59
"""

import numpy as np
import pandas as pd
import random
import tensorflow as tf
import tensorflow_hub as hub
import gdown
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from nltk.corpus import stopwords
from collections import Counter

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

url = 'https://raw.githubusercontent.com/ezzybala/NLP-task/main/new_search_data.csv''

output_path = '/content/all_tickets_processed.csv'

gdown.download(url, output_path, quiet=False)

data = pd.read_csv(output_path)

df = pd.read_csv('/content/all_tickets_processed.csv')

# # shuffle the DataFrame rows
df = df.sample(frac=1, random_state=28)

df.head()

df['Topic_group'].value_counts()

df['Topic_group'].unique()

df['Topic_group'].value_counts().plot(kind='bar')

# Document Length Analysis
print('Average number of characters:', df['Document'].apply(len).mean())
print('Average number of words:', df['Document'].apply(lambda x: len(x.split())).mean())

"""To balance out the dataset, downsample the majority classes (Hardware, HR Support, Access, Miscellaneous) such that there are only 5000 rows each."""

# Tokenize the text and split it into words
tokenized_text = df['Document'].apply(lambda x: x.split())

# Flatten the list of tokenized words
all_words = [word for sublist in tokenized_text for word in sublist]

# Remove stop words from the list of words
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in all_words if word.lower() not in stop_words]

# Count the occurrences of each word
word_counts = Counter(filtered_words)

# Create a DataFrame from the word counts
stop_words_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Count'])

# Sort the DataFrame by count in descending order
stop_words_df = stop_words_df.sort_values(by='Count', ascending=False)
stop_words_df

from matplotlib import pyplot as plt
stop_words_df['Count'].plot(kind='line', figsize=(8, 4), title='Count')
plt.gca().spines[['top', 'right']].set_visible(False)

# Get the indices for each category
hardware_indices = df[df['Topic_group'] == 'Hardware'].index
hr_support_indices = df[df['Topic_group'] == 'HR Support'].index
access_indices = df[df['Topic_group'] == 'Access'].index
miscellaneous_indices = df[df['Topic_group'] == 'Miscellaneous'].index

# Create a list of indices to drop
indices_to_drop = random.sample(list(hardware_indices), len(hardware_indices) - 5000) + \
                  random.sample(list(hr_support_indices), len(hr_support_indices) - 5000) + \
                  random.sample(list(access_indices), len(access_indices) - 5000) + \
                  random.sample(list(miscellaneous_indices), len(miscellaneous_indices) - 5000)

# Drop the rows
tickets_df_downsample = df.drop(indices_to_drop)

# Check the number of rows for each category
print(tickets_df_downsample.shape)
print("Hardware count:", tickets_df_downsample[tickets_df_downsample['Topic_group'] == 'Hardware'].shape[0])
print("HR Support count:", tickets_df_downsample[tickets_df_downsample['Topic_group'] == 'HR Support'].shape[0])
print("Access count:", tickets_df_downsample[tickets_df_downsample['Topic_group'] == 'Access'].shape[0])
print("Miscellaneous count:", tickets_df_downsample[tickets_df_downsample['Topic_group'] == 'Miscellaneous'].shape[0])

tickets_df_downsample['Topic_group'].value_counts().plot(kind='bar')

import re
def cleanTxt(text):
    text = re.sub(r'@\w+|#\w+', '', text) #Removing @mentions
    text = re.sub('#', '', text) # Removing '#' hash tag
    text = re.sub('RT[\s]+', '', text) # Removing RT
    text = re.sub('\w+:\/\/\S+', '', text) # Removing hyperlink
    #remove punctuation
    text = re.sub('[^a-zA-Z]', ' ', text)
    text.lower()
    return text

tickets_df_downsample['Cleaned_Document'] = tickets_df_downsample['Document'].apply(lambda x: cleanTxt(x))

tickets_df_downsample['Cleaned_Document'] = tickets_df_downsample['Cleaned_Document'].apply(lambda x: x.lower())

import nltk
# download all

nltk.download('all')

from nltk.corpus import stopwords

stop = stopwords.words('english')

def remove_stopwords(text):
    text = [word for word in text.split() if word not in stop]
    return " ".join(text)

# stopword removal

tickets_df_downsample['Cleaned_Document'] = tickets_df_downsample['Cleaned_Document'].apply(lambda x: remove_stopwords(x))

# function to lemmitize Topic_group column

def lemmatization(text):
    text = [lemmatizer.lemmatize(word) for word in text.split()]
    return " ".join(text)

# lemmatization
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

tickets_df_downsample['Cleaned_Document'] = tickets_df_downsample['Cleaned_Document'].apply(lambda x: lemmatization(x))

# Assuming 'df' is your DataFrame and 'columns_to_drop' is a list of column names you want to drop
columns_to_drop = ['Document']

# Drop the specified columns
tickets_df_downsample.drop(columns=columns_to_drop, inplace=True)

# Define a dictionary to map labels to numerical values
label_mapping = {
    'Access': 0,
    'HR Support': 1,
    'Hardware': 2,
    'Miscellaneous': 3,
    'Storage': 4,
    'Purchase': 5,
    'Internal Project': 6,
    'Administrative rights': 7
}

# Map the labels to their corresponding numerical values
tickets_df_downsample['Encoded_Labels'] = tickets_df_downsample['Topic_group'].map(label_mapping)

tickets_df_downsample

y=tickets_df_downsample['Encoded_Labels']
X = tickets_df_downsample['Cleaned_Document']

from sklearn.model_selection import train_test_split


# Split the dataset into training and test sets (80% training, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Step 2: Further split the training set into training and validation sets (75% training, 25% validation)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=42)

# Print the shapes of the training and test sets
print("Training set - Features:", X_train.shape, " Labels:", y_train.shape)
print("Test set - Features:", X_test.shape, " Labels:", y_test.shape)

# x_val = X_train[:5000]
# partial_x_train = X_train[18000:]

# y_val = y_train[:5000]
# partial_y_train = y_train[18000:]

# model = "https://tfhub.dev/google/nnlm-en-dim50/2"
# hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)
# hub_layer(X[:3])

model = "https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2"
hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)
hub_layer(X[:3])

# model = "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2"
# hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)
# hub_layer(X[:3])

# # Define the model
# model = tf.keras.Sequential([
#     hub_layer,
#     tf.keras.layers.Dense(16, activation='relu'),
#     tf.keras.layers.Dense(3, activation='softmax')  # Change num_classes to the number of classes in your dataset
# ])

model = tf.keras.Sequential([
    hub_layer,
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(rate=0.5),  # Dropout layer
    tf.keras.layers.Dense(8, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Use sparse categorical crossentropy for multi-class classification
              metrics=['accuracy'])  # Add additional metrics if needed

# #Trying to adjust the learning rate
# # Define the optimizer with a custom learning rate
# optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)  # Adjust the learning rate as needed

# # Compile the model with the optimizer
# model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train,
                    y_train,
                    epochs=14,
                    batch_size=720,
                    validation_data=(X_val, y_val),
                    verbose=1)

results = model.evaluate(X_test, y_test)

print(results)

results_pred = model.predict(X_test)
classes_x=np.argmax(results_pred ,axis=1)

history_dict = history.history
history_dict.keys()

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()



conf_matrix = confusion_matrix(y_test, classes_x)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score, f1_score, ConfusionMatrixDisplay


# Accuracy
accuracy = accuracy_score(y_test, classes_x)

# Precision
precision = precision_score(y_test, classes_x, average='weighted')

# Recall
recall = recall_score(y_test, classes_x, average='weighted')

# F1-score
f1 = f1_score(y_test, classes_x, average='weighted')

# Confusion matrix
conf_matrix = confusion_matrix(y_test, classes_x)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Confusion matrix:\n", conf_matrix)


# Prepare data
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
values = [accuracy, precision, recall, f1]

# Create the plot
plt.bar(metrics, values, color=['blue', 'green', 'orange', 'red'])

# Add labels and title
plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Performance Metrics')

# Show the plot
plt.show()